{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate-speach-identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1BkLY1N0o3QjvCn2aGXrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahshawaiz/hatespeach-group-identification/blob/master/hate_speach_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_FPjdN6tKKI",
        "colab_type": "code",
        "outputId": "516f7e9f-c788-4eae-e9f5-ff98bc91783e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# # # activate kaggle, mount grdive, download/unzip dataset\n",
        "# import os\n",
        "# import glob\n",
        "\n",
        "# os.environ['KAGGLE_USERNAME'] = \"shahshawaiz\" # username from the json file\n",
        "# os.environ['KAGGLE_KEY'] = \"476e4fb10eef6d9da79358d4df0db985\" # key from the json file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # init consts\n",
        "# PATH_HOME = \"/content\"\n",
        "# PATH_GDRIVE = '/content/drive/My Drive/Colab Notebooks/dataset/hate-speach/train'\n",
        "# FILE_TYPE = {\n",
        "#     \"1\": \"datasets\",\n",
        "#     \"2\": \"competitions\"\n",
        "# }\n",
        "# FILES = {\n",
        "#     FILE_TYPE[\"1\"] : [\n",
        "#         \"jdpaletto/glove-global-vectors-for-word-representation\"\n",
        "#     ],\n",
        "#     FILE_TYPE[\"2\"]: [\n",
        "#         \"jigsaw-unintended-bias-in-toxicity-classification\"\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# # iterate files and download\n",
        "# for file_type, paths in FILES.items():\n",
        "#   for path in paths:\n",
        "#     try:\n",
        "#       # move and delete\n",
        "#       !kaggle \"$file_type\" download \"$path\"\n",
        "      \n",
        "#       archives = glob.glob(\"/content/*.zip\")\n",
        "#       for download in archives:\n",
        "#         file_dir = download.strip('.zip')\n",
        "#         !unzip -qq \"$download\"\n",
        "#         !mv -n \"$file_dir\" \"$PATH_GDRIVE\"/\n",
        "#         !mv -n \"$file_dir\"/ \"$PATH_GDRIVE\"/\n",
        "#         !rm -rf \"$download\" \"$file_dir\"\n",
        "\n",
        "#       non_archives = glob.glob(\"/content/*.txt\") + glob.glob(\"/content/*.csv\")\n",
        "#       for download in non_archives:\n",
        "#         !mv -n \"$download\" \"$PATH_GDRIVE\"/\n",
        "#         !rm -rf \"$download\"\n",
        "\n",
        "\n",
        "#     except OSError as e:\n",
        "#       print(\"failed download: \", path, file)\n",
        "#       pass\n",
        "\n",
        "# # # drive.flush_and_unmount()   \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLai9C2cEW-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pkgs installation\n",
        "# !pip install tensorflow h5py pyyaml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO7THy4XENa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "4e139273-7c45-4543-9358-d82ca4c3ddf6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.metrics import roc_curve, auc, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Activation, SpatialDropout1D, Bidirectional, CuDNNLSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import Adam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "from keras.initializers import Constant\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTzYAmj_Ed0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set env\n",
        "MODEL_VERSION = 'a'\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/dataset/hate-speach'\n",
        "PATH_GLOVE_100d = PATH + '/train/glove.twitter.27B.100d.txt'\n",
        "PATH_TRAIN = PATH + '/train/train.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jOvcBxnETwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "647acd8b-869b-45f8-901e-2b910004fa23"
      },
      "source": [
        "# glove embeding routine\n",
        "def get_glove_embedding():\n",
        "    _embeddings_index = {}\n",
        "    f = open(PATH_GLOVE_100d)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        _embeddings_index[word] = coefs\n",
        "    f.close() \n",
        "    \n",
        "    return _embeddings_index\n",
        "    \n",
        "embeddings_index = get_glove_embedding()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1193514 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BExKIL5ZFEbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "6b487f56-9487-4468-f01d-3427aebaff2f"
      },
      "source": [
        "# read csv\n",
        "train=pd.read_csv(PATH_TRAIN)\n",
        "train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>bisexual</th>\n",
              "      <th>black</th>\n",
              "      <th>buddhist</th>\n",
              "      <th>christian</th>\n",
              "      <th>female</th>\n",
              "      <th>heterosexual</th>\n",
              "      <th>hindu</th>\n",
              "      <th>homosexual_gay_or_lesbian</th>\n",
              "      <th>intellectual_or_learning_disability</th>\n",
              "      <th>jewish</th>\n",
              "      <th>latino</th>\n",
              "      <th>male</th>\n",
              "      <th>muslim</th>\n",
              "      <th>other_disability</th>\n",
              "      <th>other_gender</th>\n",
              "      <th>other_race_or_ethnicity</th>\n",
              "      <th>other_religion</th>\n",
              "      <th>other_sexual_orientation</th>\n",
              "      <th>physical_disability</th>\n",
              "      <th>psychiatric_or_mental_illness</th>\n",
              "      <th>transgender</th>\n",
              "      <th>white</th>\n",
              "      <th>created_date</th>\n",
              "      <th>publication_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:41.987077+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>59849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Thank you!! This would make my life a lot less...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:42.870083+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is such an urgent design problem; kudos t...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:45.222647+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59855</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Is this something I'll be able to install on m...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:47.601894+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59856</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>haha you guys are a bunch of losers.</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.87234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2015-09-29 10:50:48.488476+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    target  ... identity_annotator_count  toxicity_annotator_count\n",
              "0  59848  0.000000  ...                        0                         4\n",
              "1  59849  0.000000  ...                        0                         4\n",
              "2  59852  0.000000  ...                        0                         4\n",
              "3  59855  0.000000  ...                        0                         4\n",
              "4  59856  0.893617  ...                        4                        47\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O0AEn_lFULg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class dist\n",
        "def plot_class_dist(df):\n",
        "    plt.bar(range(0, len(df.columns.values)), df.sum().values, align='center')\n",
        "    plt.ylabel('Classes')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.show()    \n",
        "    \n",
        "def plot_class_dist_b(col_list):\n",
        "    plt.bar(range(0, 4), col_list, align='center')\n",
        "    plt.ylabel('Classes')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTT7O5dFaUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resampling\n",
        "def try_underSampling(train):\n",
        "    \n",
        "    #Prepare for under sampling \n",
        "    religion = train[(train.religion == 1)]\n",
        "    print(\"target religion : \",religion.shape)\n",
        "    ethnicity = train[(train.ethnicity == 1)]\n",
        "    print(\"ethnicity: \",ethnicity.shape)\n",
        "    sexualOrientation = train[(train.sexualOrientation == 1)]\n",
        "    print(\"sexualOrientation: \",sexualOrientation.shape)\n",
        "    gender = train[(train.sex == 1)]\n",
        "    print(\"gender: \",gender.shape)\n",
        "    no_hatespeech = train[(train.religion == 0) | (train.ethnicity == 0) | (train.sexualOrientation == 0) | (train.sex == 0)]\n",
        "    print(\"No hatespeech : \",no_hatespeech.shape)\n",
        "    all_together = train[(train.religion == 1) | (train.ethnicity == 1) | (train.sexualOrientation == 1) | (train.sex == 1)]\n",
        "    \n",
        "    religion = religion.sample(1189, replace=True)\n",
        "    ethnicity = ethnicity.sample(1189, replace=True)\n",
        "    sexualOrientation = sexualOrientation.sample(1189, replace=True)\n",
        "    gender = gender.sample(1189, replace=True)\n",
        "    no_hatespeech = no_hatespeech.sample(1189, replace=True)\n",
        "    all_together = all_together.sample(1189, replace=True)\n",
        "\n",
        "    print(\"New target religion : \",religion.shape)\n",
        "    print(\"New ethnicity: \", ethnicity.shape)\n",
        "    print(\"New sexualOrientation: \",sexualOrientation.shape)\n",
        "    print(\"New gender: \",gender.shape)\n",
        "    print(\"New No hatespeech : \",no_hatespeech.shape)\n",
        "    print(\"All : \",all_together.shape)\n",
        "    \n",
        "    train_set = pd.concat([religion, ethnicity, sexualOrientation, gender, no_hatespeech])\n",
        "    train_set = train_set[['id', 'target', 'comment_text', 'religion','ethnicity','sexualOrientation', 'sex', 'tags']]\n",
        "    \n",
        "    return train_set, [religion.shape[0], ethnicity.shape[0], sexualOrientation.shape[0], gender.shape[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayPw9QFmFeXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding routine\n",
        "def one_hot_encode_by_label(dataset, new_col, col_list, threshold_val=0.1):\n",
        "    \n",
        "    for col in col_list:\n",
        "        dataset[new_col] = (\n",
        "            dataset[col] > threshold_val\n",
        "        )*1.0  \n",
        "    return dataset\n",
        "\n",
        "# encode all column/groups\n",
        "religions = [\n",
        "    'atheist',\n",
        "    'buddhist',\n",
        "    'christian',\n",
        "    'hindu',\n",
        "    'jewish',\n",
        "    'muslim',\n",
        "    'other_religion'\n",
        "]\n",
        "train = one_hot_encode_by_label(train, 'religion', religions)\n",
        "\n",
        "#\n",
        "ethnicity = [\n",
        "    'asian',\n",
        "    'black',\n",
        "    'latino',\n",
        "    'white',\n",
        "    'other_race_or_ethnicity'\n",
        "]\n",
        "train = one_hot_encode_by_label(train, 'ethnicity', ethnicity)\n",
        "\n",
        "# \n",
        "sexualOrientation = [\n",
        "    'bisexual',\n",
        "    'heterosexual',\n",
        "    'homosexual_gay_or_lesbian',\n",
        "    'transgender',\n",
        "    'other_sexual_orientation'    \n",
        "]\n",
        "train = one_hot_encode_by_label(train, 'sexualOrientation', sexualOrientation)\n",
        "\n",
        "#\n",
        "sex = [\n",
        "    'male',\n",
        "    'female',\n",
        "    'other_gender' \n",
        "]\n",
        "\n",
        "# run one hot encoding\n",
        "train = one_hot_encode_by_label(train, 'sex', sex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdcuZfL4Fh-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add tag column\n",
        "train[\"tags\"] = 0\n",
        "train[\"tags\"] = train.apply(lambda x: x['tags'] if x['religion']==0 else x['religion']*0, axis=1)\n",
        "train[\"tags\"] = train.apply(lambda x: x['tags'] if x['ethnicity']==0 else x['ethnicity']*1, axis=1)\n",
        "train[\"tags\"] = train.apply(lambda x: x['tags'] if x['sexualOrientation']==0 else x['sexualOrientation']*2, axis=1)\n",
        "train[\"tags\"] = train.apply(lambda x: x['tags'] if x['sex']==0 else x['sex']*3, axis=1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZI3xcdQFloV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "outputId": "a525861e-06c9-4c1a-fd36-77b8ca84ab51"
      },
      "source": [
        "# filter data with only hate comments\n",
        "train=train[(train.religion == 1) | (train.ethnicity == 1) | (train.sexualOrientation == 1) | (train.sex == 1)]\n",
        "\n",
        "\n",
        "# run resampling\n",
        "plot_class_dist(train[['religion','ethnicity','sexualOrientation', 'sex']])\n",
        "train, sample_shape = try_underSampling(train)\n",
        "\n",
        "train[['religion','ethnicity','sexualOrientation', 'sex']].sum(axis=0).values\n",
        "\n",
        "plot_class_dist_b(sample_shape)\n",
        "\n",
        "# \n",
        "train.tail()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEICAYAAAB1f3LfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "target religion :  (8404, 50)\n",
            "ethnicity:  (12094, 50)\n",
            "sexualOrientation:  (1953, 50)\n",
            "gender:  (1189, 50)\n",
            "No hatespeech :  (22679, 50)\n",
            "New target religion :  (1189, 50)\n",
            "New ethnicity:  (1189, 50)\n",
            "New sexualOrientation:  (1189, 50)\n",
            "New gender:  (1189, 50)\n",
            "New No hatespeech :  (1189, 50)\n",
            "All :  (1189, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEICAYAAABMGMOEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>religion</th>\n",
              "      <th>ethnicity</th>\n",
              "      <th>sexualOrientation</th>\n",
              "      <th>sex</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>674625</th>\n",
              "      <td>1066858</td>\n",
              "      <td>0.3</td>\n",
              "      <td>\"Looking at our surveys over time, it has beco...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650464</th>\n",
              "      <td>6145087</td>\n",
              "      <td>0.2</td>\n",
              "      <td>He's the Commander in Chief of the Armed Servi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231655</th>\n",
              "      <td>526268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I think we need to consider Hillary Clinton's ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733375</th>\n",
              "      <td>5018988</td>\n",
              "      <td>0.0</td>\n",
              "      <td>People don't want to be priests in general. Op...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52117</th>\n",
              "      <td>305950</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True dat. Here's another conundrum: why do we ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  target  ...  sex  tags\n",
              "674625   1066858     0.3  ...  0.0   2.0\n",
              "1650464  6145087     0.2  ...  0.0   1.0\n",
              "231655    526268     0.0  ...  0.0   2.0\n",
              "733375   5018988     0.0  ...  0.0   0.0\n",
              "52117     305950     0.0  ...  0.0   2.0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h2_XBJEF4QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x,y sepration\n",
        "X = train['comment_text']\n",
        "y = train[['religion','ethnicity','sexualOrientation', 'sex']]\n",
        "train.columns\n",
        "y2 = train['tags']\n",
        "\n",
        "# # train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, stratify=y2, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6neO-mp1F8JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train[[ 'religion','ethnicity','sexualOrientation', 'sex']].values\n",
        "y_test = y_test[[ 'religion','ethnicity','sexualOrientation', 'sex']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCaQsUg3F_Kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init vars\n",
        "max_features = 20000  # number of words we want to keep\n",
        "maxlen = 200  # max length of the comments in the model\n",
        "batch_size = 64  # batch size for the model\n",
        "embedding_dims = 100  # dimension of the hidden variable, i.e. the embedding dimension\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M9Yo-lDGBgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "848fd68c-cff2-4098-80a3-8ec962f067c6"
      },
      "source": [
        "# init tkenizer\n",
        "tok = Tokenizer(num_words=max_features)\n",
        "\n",
        "X_train_org = X_train\n",
        "X_test_org = X_test\n",
        "\n",
        "# fit train/test comments over tokenizer\n",
        "tok.fit_on_texts(list(X_train) + list(X_test))\n",
        "\n",
        "# convert text to sequence\n",
        "x_train = tok.texts_to_sequences(X_train)\n",
        "x_test = tok.texts_to_sequences(X_test)\n",
        "\n",
        "# debug\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
        "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4756 train sequences\n",
            "1189 test sequences\n",
            "Average train sequence length: 74\n",
            "Average test sequence length: 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeGxpRTdGCDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e7cde6b-321c-48e4-b365-8c372e309c2f"
      },
      "source": [
        "word_index = tok.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "num_words = min(max_features, len(word_index)) + 1\n",
        "print(num_words)\n",
        "\n",
        "# init embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dims))\n",
        "\n",
        "# for every word token, get vector from glove embeding vecotr (from w2v model)\n",
        "for word, i in word_index.items():\n",
        "    if i > max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dims)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 23732 unique tokens.\n",
            "20001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNLMu8nmGEcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "y_train=y_train[:1209265,:]\n",
        "y_test=y_test[:1209265,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kWAGks-GJJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8ee64cee-a2c3-4630-f01e-747d4900e1e5"
      },
      "source": [
        "# pad sequences to get equal length vector/comments\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print(x_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (4756, 200)\n",
            "x_test shape: (1189, 200)\n",
            "[[   0    0    0 ...   49  277 1227]\n",
            " [   0    0    0 ...   84   13   50]\n",
            " [   0    0    0 ...  699   31   13]\n",
            " ...\n",
            " [   0    0    0 ...  227    9   54]\n",
            " [   0    0    0 ...  375    9   12]\n",
            " [   0    0    0 ...    3   25  180]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzJ51wEeGL1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f1 score routine\n",
        "def f1_score_b(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9SHv-4ZGMV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# confusion matrix routine\n",
        "def get_confusion_matrix(model, x_test, y_true):\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred = (y_pred > 0.5) #greater than 0.50 on scale 0 to 1\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    return classification_report(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV6KmlsKGQZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm bidirectional with glove embeding model creation \n",
        "def create_model_feedforward(\n",
        "    f1_score=f1_score_b,\n",
        "    n_input=5,\n",
        "    n_output=4,    \n",
        "    output_activation='softmax', \n",
        "    loss='binary_crossentropy', \n",
        "    optimizer='adam'\n",
        "):\n",
        "    \n",
        "    # model init\n",
        "    model = Sequential()\n",
        "    \n",
        "    # layer embeding     \n",
        "    model.add(\n",
        "        Embedding(\n",
        "            max_features, \n",
        "            embedding_dims, \n",
        "            input_length=maxlen, \n",
        "            embeddings_initializer=\"uniform\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # layer pooling for dim reducntion\n",
        "    model.add(\n",
        "        GlobalMaxPooling1D()\n",
        "    )\n",
        "    \n",
        "    #      \n",
        "    model.add(\n",
        "        Dense(n_input, activation='relu')\n",
        "    )\n",
        "    \n",
        "    # output     \n",
        "    model.add(\n",
        "        Dense(n_output, activation=output_activation)\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy', f1_score])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S_Py3BqGdcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30a04131-a984-488a-b539-2089eb528199"
      },
      "source": [
        "model1 = create_model_feedforward()\n",
        "model1.summary()\n",
        "\n",
        "history_ff = model1.fit(x_train, y_train, verbose=1, epochs=epochs)\n",
        "\n",
        "print(get_confusion_matrix(model1,x_train, y_train))\n",
        "\n",
        "# print(X_test)\n",
        "# X_test = tok.texts_to_sequences(X_test)\n",
        "# dat_test = pd.DataFrame([\"taking jobs\", \"muslims\", \"jews\", \"christians\", \"muslims are evil people\", \"jews are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "# dat_test_test1 = tok.texts_to_sequences(dat_test)\n",
        "# x_test_1 = sequence.pad_sequences(dat_test_test1, maxlen=maxlen)\n",
        "# y_pred = model1.predict(x_test_1)\n",
        "# # y_pred_df = pd.DataFrame(y_pred)\n",
        "\n",
        "# model1_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "# model1_acc = accuracy_score(y_test2, y_pred2, normalize=True)\n",
        "# print('f1:', model1_f1, 'acc', model1_acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 100)          2000000   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 505       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 24        \n",
            "=================================================================\n",
            "Total params: 2,000,529\n",
            "Trainable params: 2,000,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "4756/4756 [==============================] - 6s 1ms/step - loss: 0.5915 - acc: 0.7179 - f1_score_b: 0.0000e+00\n",
            "Epoch 2/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.5336 - acc: 0.7383 - f1_score_b: 0.1417\n",
            "Epoch 3/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.4346 - acc: 0.8007 - f1_score_b: 0.4927\n",
            "Epoch 4/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.3499 - acc: 0.8417 - f1_score_b: 0.6346\n",
            "Epoch 5/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.2695 - acc: 0.8869 - f1_score_b: 0.7587\n",
            "Epoch 6/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.1967 - acc: 0.9246 - f1_score_b: 0.8497\n",
            "Epoch 7/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.1413 - acc: 0.9435 - f1_score_b: 0.8904\n",
            "Epoch 8/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.1054 - acc: 0.9533 - f1_score_b: 0.9107\n",
            "Epoch 9/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.0841 - acc: 0.9591 - f1_score_b: 0.9226\n",
            "Epoch 10/10\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 0.0721 - acc: 0.9614 - f1_score_b: 0.9275\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.88      0.94      1473\n",
            "           1       1.00      0.90      0.94      1638\n",
            "           2       1.00      0.82      0.90      1173\n",
            "           3       1.00      0.86      0.93      1082\n",
            "\n",
            "   micro avg       1.00      0.87      0.93      5366\n",
            "   macro avg       1.00      0.87      0.93      5366\n",
            "weighted avg       1.00      0.87      0.93      5366\n",
            " samples avg       0.98      0.93      0.95      5366\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxLA_uLmGg8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm bidirectional with glove embeding model creation \n",
        "def create_model_org_lstm_bidir_glove(\n",
        "    f1_score=f1_score_b,\n",
        "    n1_lstm=64,\n",
        "    n2_lstm=32,\n",
        "    n_output=4,\n",
        "    spatial_dropout=0.2, \n",
        "    pre_output_dropout=0.25, \n",
        "    output_neurons=4, \n",
        "    output_activation='softmax', \n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer='adam'):\n",
        "    \n",
        "    # init model\n",
        "    model = Sequential()\n",
        "    \n",
        "    # layer embeding\n",
        "    model.add(Embedding(\n",
        "        num_words, \n",
        "        embedding_dims, \n",
        "        input_length=maxlen,\n",
        "        embeddings_initializer=Constant(embedding_matrix),\n",
        "        trainable=True        \n",
        "    ))\n",
        "\n",
        "    # layer dropout for gerneralizatoin (drops entire 1D feature maps instead of individual elements)\n",
        "    model.add(SpatialDropout1D(spatial_dropout))\n",
        "    \n",
        "    # Bidirectional: ?,   CuDNNLSTM: better/fast implementation of LSTM\n",
        "    model.add(Bidirectional(CuDNNLSTM(n1_lstm, return_sequences=True)))\n",
        "    \n",
        "    #     \n",
        "    model.add(Bidirectional(CuDNNLSTM(n2_lstm)))\n",
        "    \n",
        "    # \n",
        "    model.add(Dropout(pre_output_dropout))\n",
        "    \n",
        "    # layer output\n",
        "    model.add(Dense(units=4, activation=output_activation))\n",
        "    \n",
        "    model.compile(loss=loss,\n",
        "          optimizer=optimizer,\n",
        "          metrics=['accuracy', f1_score])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8pWydj2GjbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #  lstm bidirectional with glove embeding compilation and fitting\n",
        "# model2 = create_model_org_lstm_bidir_glove()\n",
        "# model2.summary()\n",
        "# history_lstm = model2.fit(x_train, y_train, verbose=1, epochs=10)\n",
        "\n",
        "# print(get_confusion_matrix(model2,x_train, y_train))\n",
        "\n",
        "# path_write = os.path.join('../input/','model2.h5')\n",
        "# print(path_write)\n",
        "\n",
        "# outp = model2.save(path_write)\n",
        "# print(outp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gadUwYGyGmUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NB implementation\n",
        "def create_naive_bayes():\n",
        "\n",
        "#     tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "#     features = tfidf.fit_transform(train.comment_text).toarray()\n",
        "\n",
        "    model = Pipeline([('vect', CountVectorizer()),\n",
        "                   ('tfidf', TfidfTransformer()),\n",
        "                   ('clf', MultinomialNB()),\n",
        "                  ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSBombaVGoZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79bb986a-b056-4dc2-b4d6-181901a1004e"
      },
      "source": [
        "#  nb\n",
        "model3 = create_naive_bayes()\n",
        "model3.fit(X_train2, y_train2)\n",
        "\n",
        "# f2 score\n",
        "y_pred2 = model3.predict(X_test2)\n",
        "model3_f1 = f1_score(y_test2, y_pred2, average=\"weighted\")\n",
        "model3_acc = accuracy_score(y_test2, y_pred2, normalize=True)\n",
        "print('f1:', model3_f1, 'acc', model3_acc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1: 0.5114151782924601 acc 0.5508830950378469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjcuD0cwGqEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# svm implementation\n",
        "def create_svm():\n",
        "    \n",
        "    model = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', SGDClassifier(loss='hinge', penalty='l2')) # hinge loss functino for linear SVM\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkrzGHPUGtDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c6a896d-98ff-4f21-c453-8bf7d318976e"
      },
      "source": [
        "# acm\n",
        "model4 = create_svm()\n",
        "model4.fit(X_train2, y_train2)\n",
        "\n",
        "# f2 score\n",
        "y_pred2 = model4.predict(X_test2)\n",
        "model4_f1 = f1_score(y_test2, y_pred2, average='weighted')\n",
        "model4_acc = accuracy_score(y_test2, y_pred2, normalize=True)\n",
        "print('f1:', model4_f1, 'acc', model4_acc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1: 0.7137488960807645 acc 0.7140454163162321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdgkhcj6Gv0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # testing predictions\n",
        "# y_precit = model2.predict(x_test)\n",
        "# len(x_test), y_test, len(y_test), y_precit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF6vfpcfG6fA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "1b92d60a-7f1f-453d-f134-90afd79a4638"
      },
      "source": [
        "# examples: religion','ethnicity','sexualOrientation', 'sex']\n",
        "\n",
        "dat_test = pd.DataFrame([\"muslims\", \"jews\", \"christians\", \"muslims are evil people\", \"jews are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "dat_test = tok.texts_to_sequences(dat_test)\n",
        "dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "y_pred = model1.predict(dat_test)\n",
        "y_pred_df = pd.DataFrame(y_pred)\n",
        "print('relgion (1)', y_pred_df)\n",
        "\n",
        "dat_test = pd.DataFrame([\"immigrants are taking jobs\", \"immigrants should go back\", \"i hate color black\", \"i hate color white\", \"chinese are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "dat_test = tok.texts_to_sequences(dat_test)\n",
        "dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "y_pred = model1.predict(dat_test)\n",
        "y_pred_df = pd.DataFrame(y_pred)\n",
        "print('ethinicity (2)', y_pred_df)\n",
        "\n",
        "\n",
        "dat_test = pd.DataFrame([\"bisexual\", \"gay\", \"lesbian\", \"i hate homosexuals\", \"transgenders are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "dat_test = tok.texts_to_sequences(dat_test)\n",
        "dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "y_pred = model1.predict(dat_test)\n",
        "y_pred_df = pd.DataFrame(y_pred)\n",
        "print('sexual orientation (3)', y_pred_df)\n",
        "\n",
        "dat_test = pd.DataFrame([\"women\", \"men\", \"feminism\", \"Kill woman society\", \"Bloddy female group\"], columns = ['comment_text'])['comment_text']\n",
        "dat_test = tok.texts_to_sequences(dat_test)\n",
        "dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "y_pred = model1.predict(dat_test)\n",
        "y_pred_df = pd.DataFrame(y_pred)\n",
        "print('gender (4)', y_pred_df)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "relgion (1)           0         1         2         3\n",
            "0  0.052690  0.044532  0.484081  0.418697\n",
            "1  0.069367  0.064700  0.405381  0.460551\n",
            "2  0.205741  0.013537  0.521584  0.259138\n",
            "3  0.107090  0.056384  0.295459  0.541067\n",
            "4  0.035035  0.014940  0.505189  0.444837\n",
            "ethinicity (2)           0         1         2         3\n",
            "0  0.074356  0.566541  0.074653  0.284450\n",
            "1  0.066908  0.413013  0.093524  0.426555\n",
            "2  0.115317  0.188871  0.482147  0.213665\n",
            "3  0.130014  0.326274  0.227751  0.315960\n",
            "4  0.018255  0.212176  0.026043  0.743526\n",
            "sexual orientation (3)           0         1         2         3\n",
            "0  0.095277  0.267994  0.087695  0.549034\n",
            "1  0.083357  0.006154  0.598216  0.312274\n",
            "2  0.020498  0.001515  0.856634  0.121354\n",
            "3  0.052466  0.017624  0.367891  0.562019\n",
            "4  0.032760  0.008497  0.309453  0.649290\n",
            "gender (4)           0         1         2         3\n",
            "0  0.200359  0.057038  0.385332  0.357271\n",
            "1  0.182102  0.075817  0.362646  0.379435\n",
            "2  0.413099  0.109319  0.194004  0.283578\n",
            "3  0.109674  0.082784  0.195841  0.611702\n",
            "4  0.029025  0.054279  0.189484  0.727211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0-i84oKG7DJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # examples: religion','ethnicity','sexualOrientation', 'sex']\n",
        "\n",
        "# dat_test = pd.DataFrame([\"muslims\", \"jews\", \"christians\", \"muslims are evil people\", \"jews are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "# dat_test = tok.texts_to_sequences(dat_test)\n",
        "# dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "# y_pred = model2.predict(dat_test)\n",
        "# y_pred_df = pd.DataFrame(y_pred)\n",
        "# print('relgion (1)', y_pred_df)\n",
        "\n",
        "# dat_test = pd.DataFrame([\"immigrants are taking jobs\", \"immigrants should go back\", \"i hate color black\", \"i hate color white\", \"chinese are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "# dat_test = tok.texts_to_sequences(dat_test)\n",
        "# dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "# y_pred = model2.predict(dat_test)\n",
        "# y_pred_df = pd.DataFrame(y_pred)\n",
        "# print('ethinicity (2)', y_pred_df)\n",
        "\n",
        "\n",
        "# dat_test = pd.DataFrame([\"bisexual\", \"gay\", \"lesbian\", \"i hate homosexuals\", \"transgenders are stupid\"], columns = ['comment_text'])['comment_text']\n",
        "# dat_test = tok.texts_to_sequences(dat_test)\n",
        "# dat_test = sequence.pad_sequences(dat_test, maxlen=maxlen)\n",
        "# y_pred = model2.predict(dat_test)\n",
        "# y_pred_df = pd.DataFrame(y_pred)\n",
        "# print('sexual orientation (3)', y_pred_df)\n",
        "\n",
        "# dat_test = pd.DataFrame([\"women\", \"men\", \"feminism\", \"Kill woman society\", \"Bloddy female group\"], columns = ['comment_text'])['comment_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU11npUtG9L_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8c066f0b-dd7a-443c-d9ef-6b45bb729692"
      },
      "source": [
        "# Model 1 HP grid space\n",
        "n_input=[5,10,15]\n",
        "n_output=[4] \n",
        "output_activation = ['softmax', 'relu', 'sigmoid']\n",
        "optimizer = ['adam', 'sgd']\n",
        "loss=['binary_crossentropy']\n",
        "\n",
        "# # hyperparamter gird space\n",
        "hyperparamter_space = dict(\n",
        "#     n_input=n_input,\n",
        "#     n_output=n_output,\n",
        "    output_activation=output_activation,\n",
        "\toptimizer=optimizer,\n",
        "# \tloss=loss,\n",
        ")\n",
        "\n",
        "# hyperparamter optimization using scikit learn GridSearch CV\n",
        "def optimize_hyperparamters(build_fn, x_train, y_train, hyperparamter_space=hyperparamter_space):\n",
        "\n",
        "    # init classifier\n",
        "    model = KerasClassifier(\n",
        "        build_fn=build_fn, verbose=0\n",
        "    )\n",
        "\n",
        "    # gridsearch using hyperparamer space        \n",
        "    grid = GridSearchCV(\n",
        "        estimator=model, \n",
        "        param_grid=hyperparamter_space, \n",
        "        cv=3\n",
        "    )\n",
        "\n",
        "    # model fitting        \n",
        "    grid_result = grid.fit(\n",
        "        x_train, \n",
        "        y_train\n",
        "    )\n",
        "    # print best fit \n",
        "    print(\"Best Params: %s; Score: %f\" % (grid_result.best_params_, grid_result.best_score_))\n",
        "\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "\n",
        "    # print all combinations\n",
        "    for mean, param in zip(means, params):\n",
        "        print(\"%f @ %r\" % (mean, param))\n",
        "\n",
        "    return grid_result.best_params_\n",
        "\n",
        "# get optimized hyperparamter combination\n",
        "best_params = optimize_hyperparamters(\n",
        "    create_model_feedforward,\n",
        "    x_train, \n",
        "    y_train\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Params: {'optimizer': 'adam', 'output_activation': 'softmax'}; Score: 0.717935\n",
            "0.717935 @ {'optimizer': 'adam', 'output_activation': 'softmax'}\n",
            "0.717935 @ {'optimizer': 'adam', 'output_activation': 'relu'}\n",
            "0.695552 @ {'optimizer': 'adam', 'output_activation': 'sigmoid'}\n",
            "0.717935 @ {'optimizer': 'sgd', 'output_activation': 'softmax'}\n",
            "0.717935 @ {'optimizer': 'sgd', 'output_activation': 'relu'}\n",
            "0.717935 @ {'optimizer': 'sgd', 'output_activation': 'sigmoid'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoaYpDqHB72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # # lstm bidirectional with glove embeding model creation \n",
        "# # Model 2 HP grid space\n",
        "# # optimizer = ['softmax','adam', 'sgd']\n",
        "# n1_lstm = [32, 64]\n",
        "# n2_lstm = [64, 96]\n",
        "# # n_input=[5,10,15]\n",
        "# # spatial_dropout=[0.2, 0.4, 0.6]\n",
        "# # pre_output_dropout=[0.25, 0.45, 0.65],   \n",
        "# # output_neurons = [4]\n",
        "# # output_activation = ['softmax','relu', 'sigmoid']\n",
        "# # optimizer = ['adam', 'sgd']\n",
        "# # loss=['binary_crossentropy']\n",
        "\n",
        "# # # hyperparamter gird space\n",
        "# hyperparamter_space = dict(\n",
        "#     n1_lstm=n1_lstm,\n",
        "#     n2_lstm=n2_lstm,\n",
        "# #     spatial_dropout=spatial_dropout,\n",
        "# #     pre_output_dropout=pre_output_dropout\n",
        "# )\n",
        "\n",
        "# # hyperparamter optimization using scikit learn GridSearch CV\n",
        "# def optimize_hyperparamters(build_fn, x_train, y_train, hyperparamter_space=hyperparamter_space):\n",
        "\n",
        "#     # init classifier\n",
        "#     model = KerasClassifier(\n",
        "#         build_fn=build_fn, verbose=0\n",
        "#     )\n",
        "\n",
        "#     # gridsearch using hyperparamer space        \n",
        "#     grid = GridSearchCV(\n",
        "#         estimator=model, \n",
        "#         param_grid=hyperparamter_space, \n",
        "#         cv=3\n",
        "#     )\n",
        "\n",
        "#     # model fitting        \n",
        "#     grid_result = grid.fit(\n",
        "#         x_train, \n",
        "#         y_train\n",
        "#     )\n",
        "#     # print best fit \n",
        "#     print(\"Best Params: %s; Score: %f\" % (grid_result.best_params_, grid_result.best_score_))\n",
        "\n",
        "#     means = grid_result.cv_results_['mean_test_score']\n",
        "#     params = grid_result.cv_results_['params']\n",
        "\n",
        "#     # print all combinations\n",
        "#     for mean, param in zip(means, params):\n",
        "#         print(\"%f @ %r\" % (mean, param))\n",
        "\n",
        "#     return grid_result.best_params_\n",
        "\n",
        "# # get optimized hyperparamter combination\n",
        "# best_params = optimize_hyperparamters(\n",
        "#     create_model_org_lstm_bidir_glove,\n",
        "#     x_train, \n",
        "#     y_train\n",
        "# )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osBpa7DVHDqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # training history\n",
        "# df=pd.DataFrame({\n",
        "#     'epoch': range(0,history_lstm.params['epochs']), \n",
        "#     'm1': history_lstm.history['f1_score_b'], \n",
        "#     'm2': history_ff.history['f1_score_b']\n",
        "# })\n",
        " \n",
        "# plt.plot( 'epoch', 'm1', data=df, marker='', color='red', linewidth=2)\n",
        "# plt.plot( 'epoch', 'm2', data=df, marker='', color='blue', linewidth=2)\n",
        "\n",
        "# plt.title('Training History')\n",
        "# plt.ylabel('F1')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['FF', 'LSTM'], loc='upper left')\n",
        "# plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOs9d-DRHGXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # ROC/AOC curve\n",
        "# false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\n",
        "# roc_auc = auc(false_positive_rate, true_positive_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}